{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d3b2e9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ea2bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T10:18:57.157976Z",
     "start_time": "2022-03-04T10:18:37.335379Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 27.4     |\n",
      "|    ep_rew_mean        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3282     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.667   |\n",
      "|    explained_variance | -0.487   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -3.83    |\n",
      "|    value_loss         | 47.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 22.1     |\n",
      "|    ep_rew_mean        | 22.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3230     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | -0.161   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.6      |\n",
      "|    value_loss         | 8.95     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.5     |\n",
      "|    ep_rew_mean        | 20.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3226     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | -0.00897 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -9       |\n",
      "|    value_loss         | 213      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.1     |\n",
      "|    ep_rew_mean        | 20.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3224     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | 0.00344  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 6.6      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20       |\n",
      "|    ep_rew_mean        | 20       |\n",
      "| time/                 |          |\n",
      "|    fps                | 3229     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.575   |\n",
      "|    explained_variance | -0.233   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.62     |\n",
      "|    value_loss         | 7.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.9     |\n",
      "|    ep_rew_mean        | 20.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3234     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.635   |\n",
      "|    explained_variance | 0.0446   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.82     |\n",
      "|    value_loss         | 5.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 23.1     |\n",
      "|    ep_rew_mean        | 23.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3236     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.668   |\n",
      "|    explained_variance | -0.0122  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 5.39     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24.2     |\n",
      "|    ep_rew_mean        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3234     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.685   |\n",
      "|    explained_variance | 0.00587  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.27     |\n",
      "|    value_loss         | 4.94     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.2     |\n",
      "|    ep_rew_mean        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3231     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.656   |\n",
      "|    explained_variance | 0.00167  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.18     |\n",
      "|    value_loss         | 4.55     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.6     |\n",
      "|    ep_rew_mean        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3236     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.637   |\n",
      "|    explained_variance | 0.00812  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 4.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.9     |\n",
      "|    ep_rew_mean        | 25.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3242     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.587   |\n",
      "|    explained_variance | -0.00105 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.34     |\n",
      "|    value_loss         | 3.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 26.5     |\n",
      "|    ep_rew_mean        | 26.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3246     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.634   |\n",
      "|    explained_variance | 0.00286  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.964    |\n",
      "|    value_loss         | 3.39     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 28.6      |\n",
      "|    ep_rew_mean        | 28.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 3249      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.678    |\n",
      "|    explained_variance | -4.89e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.885     |\n",
      "|    value_loss         | 2.99      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.7     |\n",
      "|    ep_rew_mean        | 29.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3252     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 9.06e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 2.66     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 31.7     |\n",
      "|    ep_rew_mean        | 31.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3254     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | 4.27e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -14.8    |\n",
      "|    value_loss         | 1.6e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.6     |\n",
      "|    ep_rew_mean        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3259     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.503   |\n",
      "|    explained_variance | 0.00763  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.72     |\n",
      "|    value_loss         | 1.93     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 38.7      |\n",
      "|    ep_rew_mean        | 38.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 3261      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.391    |\n",
      "|    explained_variance | -6.56e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.582     |\n",
      "|    value_loss         | 1.59      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 41.1      |\n",
      "|    ep_rew_mean        | 41.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 3264      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.538    |\n",
      "|    explained_variance | -6.78e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.487     |\n",
      "|    value_loss         | 1.28      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.5     |\n",
      "|    ep_rew_mean        | 45.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3267     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.448   |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.518    |\n",
      "|    value_loss         | 0.985    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 48.6      |\n",
      "|    ep_rew_mean        | 48.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 3269      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.562    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.29      |\n",
      "|    value_loss         | 0.718     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m action, _state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 14\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     16\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/gym/core.py:254\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:229\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcarttrans\u001b[38;5;241m.\u001b[39mset_translation(cartx, carty)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoletrans\u001b[38;5;241m.\u001b[39mset_rotation(\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:145\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    143\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mreshape(buffer\u001b[38;5;241m.\u001b[39mheight, buffer\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    144\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monetime_geoms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr \u001b[38;5;28;01mif\u001b[39;00m return_rgb_array \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/pyglet/window/cocoa/__init__.py:296\u001b[0m, in \u001b[0;36mCocoaWindow.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mouse_cursor()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext:\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/pyglet/gl/cocoa.py:335\u001b[0m, in \u001b[0;36mCocoaContext.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflip\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nscontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflushBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py:805\u001b[0m, in \u001b[0;36mObjCBoundMethod.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the method with the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjc_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py:775\u001b[0m, in \u001b[0;36mObjCMethod.__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    773\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_callable()\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 775\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjc_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;66;03m# Convert result to python type if it is a instance or class pointer.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m==\u001b[39m ObjCInstance:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9373946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T09:44:51.791342Z",
     "start_time": "2022-03-04T09:44:49.084846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "model = A2C('MlpPolicy', 'CartPole-v1').learn(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b92c13",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566b5aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:23:32.908504Z",
     "start_time": "2022-03-04T14:23:32.846059Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_vec_env\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACER\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACER\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macktr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACKTR\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQN\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HER\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO2\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines/deepq/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlpPolicy, CnnPolicy, LnMlpPolicy, LnCnnPolicy\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_act, build_train  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQN\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines/deepq/policies.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf_layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discrete\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import ACER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6d712",
   "metadata": {},
   "source": [
    "## Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c40e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:20:51.043416Z",
     "start_time": "2022-03-04T14:20:51.043411Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "n_actions = grid_size * grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230cd87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:20:51.043911Z",
     "start_time": "2022-03-04T14:20:51.043906Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                        shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "        return observation\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "        Â©  # reward, done, info can't be included\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def close (self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc83629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:20:51.044543Z",
     "start_time": "2022-03-04T14:20:51.044536Z"
    }
   },
   "outputs": [],
   "source": [
    "class IArchitectEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    position = [(0,0),(0,1),(0,2),(2,0),(2,1),(2,2),(2,0),(2,1),(2,2)]\n",
    "    score = 0\n",
    "    \n",
    "    def __init__(self,verbose=False):\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.state = np.zeros((3,3),dtype=np.int32)\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(3,3), dtype=np.uint8)\n",
    "        \n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "        self.win = 10\n",
    "        self.failed = -10\n",
    "        self.reward = 1\n",
    "        \n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.state = np.zeros((3,3),dtype=np.int32)\n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        position = [(0,0),(0,1),(0,2),(2,0),(2,1),(2,2),(2,0),(2,1),(2,2)]\n",
    "        \n",
    "        if self.done:\n",
    "            return self.reset()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(self.state,action)\n",
    "\n",
    "        self.x, self.y = position[action]\n",
    "        reward = self.reward\n",
    "        \n",
    "        if self.state[self.x,self.y]==1:\n",
    "            reward = self.failed\n",
    "            self.done = True\n",
    "\n",
    "        self.state[self.x,self.y] = 1\n",
    "\n",
    "        if self.state.sum()==self.state.shape[0]:\n",
    "            self.done = True\n",
    "            reward = self.win\n",
    "        \n",
    "        #score += reward\n",
    "        info = {}#{f'Score:{score}'}\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(self.state, reward, self.score)\n",
    "            \n",
    "        return self.state, reward, self.done, info\n",
    "\n",
    "    \n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        print(self.state, action, self.x, self.y)\n",
    "        print(self.reward, self.done)\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d22d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:20:51.044986Z",
     "start_time": "2022-03-04T14:20:51.044980Z"
    }
   },
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left. \n",
    "    \"\"\"\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['console']}\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_pos = grid_size - 1\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(5,5), dtype=np.int32)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        done = bool(self.agent_pos == 0)\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "    \n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            raise NotImplementedError()\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        print(\".\" * self.agent_pos, end=\"\")\n",
    "        print(\"x\", end=\"\")\n",
    "        print(\".\" * (self.grid_size - self.agent_pos))\n",
    "        print(reward)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce186038",
   "metadata": {},
   "source": [
    "## Test Random Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8ff16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:20:51.045419Z",
     "start_time": "2022-03-04T14:20:51.045413Z"
    }
   },
   "outputs": [],
   "source": [
    "env = IArchitectEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "beb530e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:14:25.259699Z",
     "start_time": "2022-03-04T14:14:25.256583Z"
    }
   },
   "outputs": [],
   "source": [
    "#environment = IArchitectEnv()\n",
    "#env = gym.make(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee283ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:14:25.611561Z",
     "start_time": "2022-03-04T14:14:25.593249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 4 2 1\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]] 6 2 0\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]] 5 2 2\n",
      "1 False\n",
      "Episode:1 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 2 0 2\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]] 5 2 2\n",
      "1 False\n",
      "Episode:2 Score:-9\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 5 2 2\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]] 8 2 2\n",
      "1 False\n",
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]] 0 0 0\n",
      "1 False\n",
      "Episode:3 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 3 2 0\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]] 7 2 1\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 0]] 3 2 0\n",
      "1 False\n",
      "Episode:4 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 2 0 2\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]] 5 2 2\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 1]] 4 2 1\n",
      "1 False\n",
      "Episode:5 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 6 2 0\n",
      "1 False\n",
      "[[0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]] 2 0 2\n",
      "1 False\n",
      "[[0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]] 8 2 2\n",
      "1 False\n",
      "Episode:6 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 1 0 1\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]] 6 2 0\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 0]] 7 2 1\n",
      "1 False\n",
      "Episode:7 Score:-8\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 6 2 0\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]] 5 2 2\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 1]] 7 2 1\n",
      "1 False\n",
      "Episode:8 Score:-8\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 4 2 1\n",
      "1 False\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 1 0 1\n",
      "1 False\n",
      "[[1 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 0 0 0\n",
      "1 False\n",
      "Episode:9 Score:12\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]] 7 2 1\n",
      "1 False\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]] 4 2 1\n",
      "1 False\n",
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]] 0 0 0\n",
      "1 False\n",
      "Episode:10 Score:12\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f56a8d",
   "metadata": {},
   "source": [
    "## Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a620282f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T14:19:58.813525Z",
     "start_time": "2022-03-04T14:19:58.790403Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ACER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mACER\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ACER' is not defined"
     ]
    }
   ],
   "source": [
    "model = ACER('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dde691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9839d",
   "metadata": {},
   "source": [
    "## Save and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"ACER_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa5bafb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T10:11:33.564237Z",
     "start_time": "2022-03-04T10:11:33.559416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = ACER.load(\"ACER_model\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97193a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and test the model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
