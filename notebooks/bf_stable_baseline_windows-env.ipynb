{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91738a1-11d8-4eb2-b14a-be4d3e98323c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T11:49:43.702176Z",
     "start_time": "2022-03-07T11:49:42.005742Z"
    }
   },
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Environement\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Evaluate the environement\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Agent\n",
    "from stable_baselines3 import A2C,PPO\n",
    "\n",
    "# Policy\n",
    "from stable_baselines3.ppo import MlpPolicy, CnnPolicy\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f133dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T11:49:43.793707Z",
     "start_time": "2022-03-07T11:49:43.703649Z"
    }
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'bf/ppo/mlp/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf/ppo/mlp/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'bf/ppo/mlp/'"
     ]
    }
   ],
   "source": [
    "log_dir = \"bf/ppo/mlp/\"\n",
    "os.makedirs(log_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a613c-6f17-4617-8bef-4506218d544e",
   "metadata": {},
   "source": [
    "# Environement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f62f7",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf4bb78c-7432-4d76-b8d5-04d5f63a205d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:23:28.124737Z",
     "start_time": "2022-03-07T12:23:28.106305Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,dimension,tuiles,quotas):\n",
    "        super().__init__()\n",
    "        assert quotas.shape[0]==tuiles.shape[0]\n",
    "        self.dimension = dimension\n",
    "        self.tuiles = tuiles\n",
    "        self.quotas = quotas\n",
    "\n",
    "        self._state = np.zeros((self.dimension,),dtype=np.int8)\n",
    "\n",
    "        # /!\\ => action = especes\n",
    "        self.action_space = spaces.Discrete(self.tuiles.shape[0])\n",
    "\n",
    "        # ETAPE 1 : OBSERVATION = REMPLISSAGE (ON RAJOUTE UNE CASE POUR LES CASES VIDES)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=np.inf,\n",
    "                                            shape=(self.tuiles.shape[0]+1,),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self._episode_ended = False\n",
    "        self._next_position = self.next_position()\n",
    "        self._taux = self.taux_remplissage()\n",
    "    \n",
    "    \n",
    "    def next_position(self):\n",
    "        \"\"\"\n",
    "        Determine la prochaine position vide\n",
    "        :return: None si la grille est full\n",
    "        \"\"\"\n",
    "        pos = np.unravel_index(np.argmin(self._state),self._state.shape)\n",
    "        if self._state[pos]!=0:\n",
    "            return None\n",
    "        return pos\n",
    "    \n",
    "    \n",
    "    def taux_remplissage(self):\n",
    "        \"\"\"\n",
    "        Détermine le taux de remplissage des quotas\n",
    "        :return: [] + taux_pour les espèces\n",
    "        \"\"\"\n",
    "        taux = np.full((len(self.tuiles)+1,),0.0,dtype=np.float32) # Taux a une case de plus que quota ou tuiles\n",
    "        ind,c = np.unique(self._state,return_counts=True)\n",
    "        taux[ind] = c  # Ind sont bien compris entre 1 et n_tuiles, car lues dans state\n",
    "        mask = self.quotas!=0.0 # Pour les quotas imposés, ie différent de zeros\n",
    "        mask_tx= np.insert(mask,0,False) # Taux a une case de plus que quota. On ajout False à la fin\n",
    "        taux[mask_tx] = taux[mask_tx]/self.quotas[mask]\n",
    "        taux[~(mask_tx)] = 0.0\n",
    "\n",
    "        return taux\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Remplit la action_ième case\n",
    "        Termine si action déjà remplie\n",
    "        \"\"\"\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        # ACTION ENTRE 0 et N_Tuile-1\n",
    "        # DANS LE STATE LES ESPECES SONT REPERERES PAR i_TUILE + 1 car 0 Pour case vides\n",
    "        \n",
    "        info = {}\n",
    "        espece_vue = action + 1\n",
    "\n",
    "        self._episode_ended = False\n",
    "        reward = 0\n",
    "\n",
    "        self._state[self._next_position] = action # POSE DU LEGUME\n",
    "        new_taux = self.taux_remplissage() # NOUVEAU TAUX, LES ANCIENS SONT ENCORE DANS self._taux\n",
    "        self._next_position = self.next_position()\n",
    "\n",
    "\n",
    "        if self._taux[espece_vue]<1.0 and self.quotas[action]>0.0:\n",
    "            # ON AUGMENTE UN QUOTA A REMPLIR\n",
    "            reward = 0.1\n",
    "            if new_taux[espece_vue]>1.0:\n",
    "                # ON LE REMPLI COMPLETEMENT\n",
    "                reward = 0.2\n",
    "        else:\n",
    "            reward = 0 # INCITATION A REMPLIR DES TAUX PROCHES DE PLEINS\n",
    "        if self._next_position is None:\n",
    "            reward = 0.5\n",
    "            self._episode_ended = True\n",
    "\n",
    "        self._taux = new_taux # ON ENREGISTRE LES NOUVEAUX TAUX\n",
    "            \n",
    "        return self.to_observation(), reward, self._episode_ended, info\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = np.random.randint(1,len(self.tuiles)+1,(self.dimension,),np.int8)\n",
    "\n",
    "        nb_zeros = np.random.randint(0,self.dimension)\n",
    "        zero_indices = np.random.randint(0,self.dimension,(nb_zeros,))\n",
    "        self._state[zero_indices] = 0\n",
    "\n",
    "        self._next_position = self.next_position()\n",
    "        if self._next_position is None:\n",
    "            return self.reset()\n",
    "\n",
    "        self._taux = self.taux_remplissage()\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return self.to_observation()\n",
    "    \n",
    "    \n",
    "    def to_observation(self):\n",
    "        return self._taux.copy()\n",
    "    \n",
    "    \n",
    "    def render(self,mode=\"human\"):\n",
    "        grill = self._state.reshape((int(self.dimension**0.5),int(self.dimension**0.5)))\n",
    "        img = np.full((grill.shape[0]*16,grill.shape[1]*256),255)\n",
    "        for r,c in itertools.product(range(grill.shape[0]),range(grill.shape[1])):\n",
    "            img[r*16:r*16+16,c*16:c*16+16] = 255-grill[r,c]*255\n",
    "\n",
    "        return img.astype('uint8')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529c91d",
   "metadata": {},
   "source": [
    "## Instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3fa00fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:23:28.605771Z",
     "start_time": "2022-03-07T12:23:28.602804Z"
    }
   },
   "outputs": [],
   "source": [
    "dimension = 25\n",
    "tuiles = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "quotas = np.array([0,3,4,0,1,0,3,4,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3834478e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:23:28.824362Z",
     "start_time": "2022-03-07T12:23:28.820059Z"
    }
   },
   "outputs": [],
   "source": [
    "env = CustomEnv(dimension,tuiles,quotas)\n",
    "eval_env = CustomEnv(dimension,tuiles,quotas) # Use a separate environement for evaluation\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679a7a5-a0ec-443f-8f36-def1a7d5b0e7",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9204df5d-b3e2-4cf1-a5c8-25ef63a9a0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:23:30.923452Z",
     "start_time": "2022-03-07T12:23:30.882808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "algo_with_policy = PPO(\"MlpPolicy\", env, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c225c-5193-45e2-8b74-77ce4f36dccd",
   "metadata": {},
   "source": [
    "## Evaluate initial Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a269e3f-3494-47bd-ae81-a18648ef096e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T13:47:18.956662Z",
     "start_time": "2022-03-07T13:45:33.061484Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Random Agent, before training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo_with_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     86\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(observations, state\u001b[38;5;241m=\u001b[39mstates, episode_start\u001b[38;5;241m=\u001b[39mepisode_starts, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m---> 87\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     89\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/iarchitect/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_position] \u001b[38;5;241m=\u001b[39m action \u001b[38;5;66;03m# POSE DU LEGUME\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m new_taux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtaux_remplissage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# NOUVEAU TAUX, LES ANCIENS SONT ENCORE DANS self._taux\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_position()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taux[espece_vue]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotas[action]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# ON AUGMENTE UN QUOTA A REMPLIR\u001b[39;00m\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mCustomEnv.taux_remplissage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m mask_tx\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(mask,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Taux a une case de plus que quota. On ajout False à la fin\u001b[39;00m\n\u001b[1;32m     49\u001b[0m taux[mask_tx] \u001b[38;5;241m=\u001b[39m taux[mask_tx]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotas[mask]\n\u001b[0;32m---> 50\u001b[0m taux[\u001b[38;5;241;43m~\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmask_tx\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m taux\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(algo_with_policy, eval_env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35099588-4b64-4539-acc6-ef5b6bb1fd84",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adb86b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T11:57:08.225878Z",
     "start_time": "2022-03-07T11:57:08.225870Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCallBack(BaseCallback):\n",
    "    def __init__(self,log_dir,steps_to_print,verbose=0):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.steps_to_print = steps_to_print\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.steps_to_print == 0:\n",
    "            plt.imshow(self.locals[\"obs_tensor\"].numpy()[0,:,:])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be0f6ba6-8938-430d-b4d2-d07159629ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T11:57:12.243563Z",
     "start_time": "2022-03-07T11:57:12.230703Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyCallBack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20_000\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the agent for 10000 steps\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[43mMyCallBack\u001b[49m(log_dir,print_steps)\n\u001b[1;32m      6\u001b[0m algo_with_policy\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mn_steps,callback\u001b[38;5;241m=\u001b[39mcallback)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyCallBack' is not defined"
     ]
    }
   ],
   "source": [
    "print_steps = 1\n",
    "n_steps = 20_000\n",
    "\n",
    "# Train the agent for 10000 steps\n",
    "callback = MyCallBack(log_dir,print_steps)\n",
    "algo_with_policy.learn(total_timesteps=n_steps,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29a289",
   "metadata": {},
   "source": [
    "## Evaluate agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb921820-fd85-4cfa-b383-259727d71322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T23:33:35.495396Z",
     "start_time": "2022-03-04T23:33:35.129692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-99.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(algo_with_policy, env, n_eval_episodes=1000)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505e210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
